{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn \n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "y=5+1.2 x-3.4 \\frac{x^2}{2!}+5.6 \\frac{x^3}{3!}+\\epsilon \\text { where } \\epsilon \\sim \\mathcal{N}\\left(0,0.1^2\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_degree = 20 #多项式的最大阶数\n",
    "n_train,n_test = 100,100 #训练数据集和测试数据集的大小\n",
    "true_w = np.zeros(max_degree) \n",
    "true_w[0:4] = np.array([5,1.2,-3.4,5.6])#前四个系数，后十六个系数是0\n",
    "\n",
    "features = np.random.normal(size=(n_train+n_test,1)) #列向量\n",
    "np.random.shuffle(features)\n",
    "poly_features = np.power(features,np.arange(max_degree).reshape(1,-1))#基数数组是列向量，指数数组是二维数组行向量，计算[[x1^0,x1^1,...],[x2^0,x2^1,...],...],这里用到了广播机制\n",
    "for i in range(max_degree):\n",
    "    poly_features[:,i]/=math.gamma(i+1) #gamma(n)=(n-1)!点除以,[:,i]每一行第i列，切片技术，/=math.gamma(i+1)运用到了广播机制\n",
    "#labels的维度: (n_train+n_test,)\n",
    "labels = np.dot(poly_features,true_w)\n",
    "labels += np.random.normal(scale=0.1,size=labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numpy array 转换成tensor\n",
    "true_w,features,poly_features,labels = [torch.tensor(x,dtype=torch.float32) for x in [true_w,features,poly_features,labels]]\n",
    "features[:2],poly_features[:2,:],labels[:2]#[:2]取前两行，[:2,:]取前两行，行是第0个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_loss(net,data_iter,loss):\n",
    "    metric = d2l.Accumulator(2)\n",
    "    for X,y in data_iter:\n",
    "        out = net(X)\n",
    "        y = y.reshape(out.shape)\n",
    "        l = loss(out,y)\n",
    "        metric.add(l.sum(),l.numel())\n",
    "    return metric[0]/metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l.torch import Animator\n",
    "class Accumulator:\n",
    "    # 在n个变量上累加\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0]*n  # 生成一个n个元素的列表\n",
    "\n",
    "    def add(self, *args):\n",
    "        self.data = [a+float(b) for a, b in zip(self.data, args)]\n",
    "\n",
    "    def reset(self):\n",
    "        self.data([0.0]*len(len(self.data)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "#分类精度,数据集的某一批的分类的准确率\n",
    "def accuracy(y_hat,y):\n",
    "    #计算预测正确的数量\n",
    "    if len(y_hat.shape)>1 and y_hat.shape[1]>1:\n",
    "       # print(y_hat.argmax(axis=1))\n",
    "        y_hat = y_hat.argmax(axis=1)\n",
    "    cmp = y_hat.type(y.dtype) == y\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "\n",
    "# 计算数据集上的精度\n",
    "def evaluate_accuracy(net, data_iter):\n",
    "    # 计算指定数据集上的精度\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.eval()  # 将模型设置为评估模式 训练模式下：Batch Normalization 层会根据当前 mini-batch 的统计数据（均值和方差）进行归一化。评估模式下：Batch Normalization 层使用整个训练过程中的全局统计数据（均值和方差）进行归一化。\n",
    "    metric = Accumulator(2)  # 正确预测数，预测总数\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0]/metric[1]\n",
    "\n",
    "\n",
    "def train_epoch_ch3(net, train_iter, loss, updater):\n",
    "    # 训练模型一轮\n",
    "    # 将模型设置为训练模式，这里的ch3是第三章的意思，李沐已经把他封装在d2l库中了\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()  # 训练模式下：Batch Normalization 层会根据当前 mini-batch 的统计数据（均值和方差）进行归一化。评估模式下：Batch Normalization 层使用整个训练过程中的全局统计数据（均值和方差）进行归一化。\n",
    "    metric = Accumulator(3)  # 列表三个元素依次是训练损失总和，训练准确度总和，样本数\n",
    "    for X, y in train_iter:\n",
    "        # 计算梯度并且更新参数\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            # 使用pytorch的内置的优化器和损失函数\n",
    "            updater.zero_grad()\n",
    "            l.mean().backward()\n",
    "            updater.step()\n",
    "        else:\n",
    "            # 使用定制的优化器和损失函数\n",
    "            l.sum().backward()\n",
    "            updater(X.shape[0])\n",
    "        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())\n",
    "    return metric[0]/metric[2], metric[1]/metric[2]\n",
    "\n",
    "\n",
    "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):\n",
    "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9], legend=[\n",
    "                        'train loss', 'train acc', 'test acc'])  # 训练损失函数，训练准确性，测试准确性\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
    "        test_acc = evaluate_accuracy(net, test_iter)\n",
    "        animator.add(epoch+1, train_metrics+(test_acc,))\n",
    "    train_loss, train_acc = train_metrics\n",
    "    assert train_loss < 0.5, train_loss\n",
    "    assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
    "    assert test_acc <= 1 and test_acc > 0.7, test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_features, test_features, train_labels, test_labels,\n",
    "          num_epochs=400):\n",
    "    loss = nn.MSELoss()\n",
    "    input_shape = train_features.shape[-1]\n",
    "    net = nn.Sequential(nn.Linear(input_shape, 1, bias=False))\n",
    "    batch_size = min(10, train_labels.shape[0])\n",
    "    train_iter = d2l.load_array((train_features, train_labels.reshape(-1, 1)),\n",
    "                                batch_size)\n",
    "    test_iter = d2l.load_array((test_features, test_labels.reshape(-1, 1)),\n",
    "                               batch_size, is_train=False)\n",
    "    trainer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss', yscale='log',\n",
    "                            xlim=[1, num_epochs], ylim=[1e-3, 1e2],\n",
    "                            legend=['train', 'test'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_epoch_ch3(net, train_iter, loss, trainer)\n",
    "        if epoch == 0 or (epoch + 1) % 20 == 0:\n",
    "            animator.add(epoch + 1, (evaluate_loss(\n",
    "                net, train_iter, loss), evaluate_loss(net, test_iter, loss)))\n",
    "    print('weight:', net[0].weight.data.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#正常拟合\n",
    "train(poly_features[:n_train,:4],poly_features[n_train:,:4],labels[:n_train],labels[n_train:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#欠拟合\n",
    "print(poly_features[:n_train,:].shape,poly_features[n_train:,:2].shape,labels[:n_train].shape,labels[n_train:].shape)\n",
    "train(poly_features[:n_train,:],poly_features[n_train:,:2],labels[:n_train],labels[n_train:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#过拟合\n",
    "train(poly_features[:n_train,:],poly_features[n_train:,:],labels[:n_train],labels[n_train:],num_epochs=1500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9ebf9cfd872009544a161647ac82c48f4cc096aba58631b69e515c7576d66293"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
